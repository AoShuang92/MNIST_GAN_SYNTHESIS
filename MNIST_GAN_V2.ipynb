{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_GAN_V2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCIRrtUHNGh6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvqoOdN_NX3N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bs = 100\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5))])\n",
        "#mnist = torchvision.datasets.MNIST('./var',download = True)\n",
        "train_dataset = torchvision.datasets.MNIST('./var', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST('./var', train=False, transform=transform, download=True)\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJMINPGPxF7_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad781ace-b9f6-4c2c-9c86-a057a883ae0c"
      },
      "source": [
        "len(train_loader)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "600"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlwRQzdgOcfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, g_input_dim, g_output_dim):\n",
        "        super(Generator, self).__init__()       \n",
        "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x): \n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        return torch.tanh(self.fc4(x))\n",
        "    \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, d_input_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
        "    \n",
        "    # forward method\n",
        "    def forward(self, x):\n",
        "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        return torch.sigmoid(self.fc4(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L6bKkEemOu3q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "c78fb222-d0c6-44e1-f137-694e21de69a5"
      },
      "source": [
        "z_dim = 100\n",
        "mnist_dim = train_dataset.train_data.size(1) * train_dataset.train_data.size(2) # 28*28\n",
        "\n",
        "G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n",
        "D = Discriminator(mnist_dim).to(device)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYfA033xPoCC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "942b6cfb-7707-455d-e1aa-ae8305d49f41"
      },
      "source": [
        "train_dataset.train_data.size()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:55: UserWarning: train_data has been renamed data\n",
            "  warnings.warn(\"train_data has been renamed data\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([60000, 28, 28])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xaiGSsgtPzSG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.BCELoss() \n",
        "lr = 0.0002 \n",
        "G_optimizer = optim.Adam(G.parameters(), lr = lr)\n",
        "D_optimizer = optim.Adam(D.parameters(), lr = lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCwcuA0DQZQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def D_train(x):\n",
        "    #=======================Train the discriminator=======================#\n",
        "    D.zero_grad()\n",
        "\n",
        "    # train discriminator on real\n",
        "    x_real, y_real = x.view(-1, mnist_dim), torch.ones(bs, 1)\n",
        "    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n",
        "\n",
        "    D_output = D(x_real)\n",
        "    D_real_loss = criterion(D_output, y_real)\n",
        "    D_real_score = D_output\n",
        "\n",
        "    # train discriminator on fake\n",
        "    z = Variable(torch.randn(bs, z_dim).to(device))\n",
        "    x_fake, y_fake = G(z), Variable(torch.zeros(bs, 1).to(device))\n",
        "\n",
        "    D_output = D(x_fake)\n",
        "    D_fake_loss = criterion(D_output, y_fake)\n",
        "    D_fake_score = D_output\n",
        "\n",
        "    # gradient backprop & optimize ONLY D's parameters\n",
        "    D_loss = D_real_loss + D_fake_loss\n",
        "    D_loss.backward()\n",
        "    D_optimizer.step()\n",
        "        \n",
        "    return  D_loss.data.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W__ihnuPtpul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def G_train(x):\n",
        "    #=======================Train the generator=======================#\n",
        "    G.zero_grad()\n",
        "\n",
        "    z = Variable(torch.randn(bs, z_dim).to(device))\n",
        "    y = Variable(torch.ones(bs, 1).to(device))\n",
        "\n",
        "    G_output = G(z)\n",
        "    D_output = D(G_output)\n",
        "    G_loss = criterion(D_output, y)\n",
        "\n",
        "    # gradient backprop & optimize ONLY G's parameters\n",
        "    G_loss.backward()\n",
        "    G_optimizer.step()\n",
        "        \n",
        "    return G_loss.data.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nk84cMuJt5TM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f702ff39-b9ef-4ab0-ae0b-a5259c21cb26"
      },
      "source": [
        "n_epoch = 200\n",
        "for epoch in range(1, n_epoch+1):           \n",
        "    D_losses, G_losses = [], []\n",
        "    for batch_idx, (x, _) in enumerate(train_loader):\n",
        "        #print(len(batch_idx))\n",
        "        D_losses.append(D_train(x))\n",
        "        G_losses.append(G_train(x))\n",
        "\n",
        "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
        "            (epoch), n_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/200]: loss_d: 0.594, loss_g: 4.985\n",
            "[2/200]: loss_d: 0.770, loss_g: 3.683\n",
            "[3/200]: loss_d: 0.734, loss_g: 2.523\n",
            "[4/200]: loss_d: 0.424, loss_g: 3.464\n",
            "[5/200]: loss_d: 0.439, loss_g: 3.415\n",
            "[6/200]: loss_d: 0.443, loss_g: 3.051\n",
            "[7/200]: loss_d: 0.458, loss_g: 3.299\n",
            "[8/200]: loss_d: 0.516, loss_g: 2.927\n",
            "[9/200]: loss_d: 0.519, loss_g: 2.809\n",
            "[10/200]: loss_d: 0.600, loss_g: 2.599\n",
            "[11/200]: loss_d: 0.676, loss_g: 2.278\n",
            "[12/200]: loss_d: 0.686, loss_g: 2.257\n",
            "[13/200]: loss_d: 0.744, loss_g: 2.033\n",
            "[14/200]: loss_d: 0.751, loss_g: 2.067\n",
            "[15/200]: loss_d: 0.806, loss_g: 1.910\n",
            "[16/200]: loss_d: 0.822, loss_g: 1.857\n",
            "[17/200]: loss_d: 0.830, loss_g: 1.789\n",
            "[18/200]: loss_d: 0.866, loss_g: 1.766\n",
            "[19/200]: loss_d: 0.846, loss_g: 1.826\n",
            "[20/200]: loss_d: 0.866, loss_g: 1.722\n",
            "[21/200]: loss_d: 0.891, loss_g: 1.632\n",
            "[22/200]: loss_d: 0.927, loss_g: 1.596\n",
            "[23/200]: loss_d: 0.906, loss_g: 1.615\n",
            "[24/200]: loss_d: 0.941, loss_g: 1.541\n",
            "[25/200]: loss_d: 0.947, loss_g: 1.535\n",
            "[26/200]: loss_d: 0.968, loss_g: 1.462\n",
            "[27/200]: loss_d: 0.982, loss_g: 1.452\n",
            "[28/200]: loss_d: 1.000, loss_g: 1.408\n",
            "[29/200]: loss_d: 1.006, loss_g: 1.403\n",
            "[30/200]: loss_d: 0.991, loss_g: 1.458\n",
            "[31/200]: loss_d: 0.972, loss_g: 1.473\n",
            "[32/200]: loss_d: 0.997, loss_g: 1.424\n",
            "[33/200]: loss_d: 1.031, loss_g: 1.337\n",
            "[34/200]: loss_d: 1.039, loss_g: 1.329\n",
            "[35/200]: loss_d: 1.041, loss_g: 1.316\n",
            "[36/200]: loss_d: 1.047, loss_g: 1.321\n",
            "[37/200]: loss_d: 1.054, loss_g: 1.293\n",
            "[38/200]: loss_d: 1.082, loss_g: 1.245\n",
            "[39/200]: loss_d: 1.057, loss_g: 1.303\n",
            "[40/200]: loss_d: 1.060, loss_g: 1.280\n",
            "[41/200]: loss_d: 1.094, loss_g: 1.228\n",
            "[42/200]: loss_d: 1.111, loss_g: 1.192\n",
            "[43/200]: loss_d: 1.105, loss_g: 1.198\n",
            "[44/200]: loss_d: 1.120, loss_g: 1.194\n",
            "[45/200]: loss_d: 1.121, loss_g: 1.171\n",
            "[46/200]: loss_d: 1.144, loss_g: 1.139\n",
            "[47/200]: loss_d: 1.149, loss_g: 1.123\n",
            "[48/200]: loss_d: 1.150, loss_g: 1.118\n",
            "[49/200]: loss_d: 1.159, loss_g: 1.104\n",
            "[50/200]: loss_d: 1.160, loss_g: 1.113\n",
            "[51/200]: loss_d: 1.152, loss_g: 1.110\n",
            "[52/200]: loss_d: 1.173, loss_g: 1.072\n",
            "[53/200]: loss_d: 1.173, loss_g: 1.078\n",
            "[54/200]: loss_d: 1.176, loss_g: 1.073\n",
            "[55/200]: loss_d: 1.182, loss_g: 1.062\n",
            "[56/200]: loss_d: 1.185, loss_g: 1.052\n",
            "[57/200]: loss_d: 1.183, loss_g: 1.042\n",
            "[58/200]: loss_d: 1.183, loss_g: 1.060\n",
            "[59/200]: loss_d: 1.195, loss_g: 1.031\n",
            "[60/200]: loss_d: 1.194, loss_g: 1.024\n",
            "[61/200]: loss_d: 1.189, loss_g: 1.053\n",
            "[62/200]: loss_d: 1.191, loss_g: 1.043\n",
            "[63/200]: loss_d: 1.194, loss_g: 1.031\n",
            "[64/200]: loss_d: 1.199, loss_g: 1.024\n",
            "[65/200]: loss_d: 1.192, loss_g: 1.041\n",
            "[66/200]: loss_d: 1.193, loss_g: 1.044\n",
            "[67/200]: loss_d: 1.204, loss_g: 1.016\n",
            "[68/200]: loss_d: 1.210, loss_g: 1.010\n",
            "[69/200]: loss_d: 1.211, loss_g: 1.009\n",
            "[70/200]: loss_d: 1.209, loss_g: 1.007\n",
            "[71/200]: loss_d: 1.211, loss_g: 1.003\n",
            "[72/200]: loss_d: 1.224, loss_g: 0.985\n",
            "[73/200]: loss_d: 1.213, loss_g: 0.991\n",
            "[74/200]: loss_d: 1.200, loss_g: 1.027\n",
            "[75/200]: loss_d: 1.201, loss_g: 1.019\n",
            "[76/200]: loss_d: 1.221, loss_g: 0.986\n",
            "[77/200]: loss_d: 1.236, loss_g: 0.965\n",
            "[78/200]: loss_d: 1.233, loss_g: 0.970\n",
            "[79/200]: loss_d: 1.230, loss_g: 0.983\n",
            "[80/200]: loss_d: 1.226, loss_g: 0.974\n",
            "[81/200]: loss_d: 1.234, loss_g: 0.969\n",
            "[82/200]: loss_d: 1.223, loss_g: 0.990\n",
            "[83/200]: loss_d: 1.229, loss_g: 0.982\n",
            "[84/200]: loss_d: 1.243, loss_g: 0.959\n",
            "[85/200]: loss_d: 1.233, loss_g: 0.959\n",
            "[86/200]: loss_d: 1.242, loss_g: 0.956\n",
            "[87/200]: loss_d: 1.239, loss_g: 0.962\n",
            "[88/200]: loss_d: 1.239, loss_g: 0.964\n",
            "[89/200]: loss_d: 1.237, loss_g: 0.963\n",
            "[90/200]: loss_d: 1.233, loss_g: 0.972\n",
            "[91/200]: loss_d: 1.249, loss_g: 0.951\n",
            "[92/200]: loss_d: 1.247, loss_g: 0.954\n",
            "[93/200]: loss_d: 1.241, loss_g: 0.964\n",
            "[94/200]: loss_d: 1.248, loss_g: 0.938\n",
            "[95/200]: loss_d: 1.256, loss_g: 0.939\n",
            "[96/200]: loss_d: 1.247, loss_g: 0.946\n",
            "[97/200]: loss_d: 1.248, loss_g: 0.942\n",
            "[98/200]: loss_d: 1.253, loss_g: 0.944\n",
            "[99/200]: loss_d: 1.250, loss_g: 0.936\n",
            "[100/200]: loss_d: 1.257, loss_g: 0.942\n",
            "[101/200]: loss_d: 1.245, loss_g: 0.961\n",
            "[102/200]: loss_d: 1.246, loss_g: 0.948\n",
            "[103/200]: loss_d: 1.244, loss_g: 0.954\n",
            "[104/200]: loss_d: 1.245, loss_g: 0.957\n",
            "[105/200]: loss_d: 1.247, loss_g: 0.951\n",
            "[106/200]: loss_d: 1.253, loss_g: 0.947\n",
            "[107/200]: loss_d: 1.255, loss_g: 0.938\n",
            "[108/200]: loss_d: 1.255, loss_g: 0.932\n",
            "[109/200]: loss_d: 1.259, loss_g: 0.927\n",
            "[110/200]: loss_d: 1.254, loss_g: 0.938\n",
            "[111/200]: loss_d: 1.255, loss_g: 0.932\n",
            "[112/200]: loss_d: 1.264, loss_g: 0.920\n",
            "[113/200]: loss_d: 1.267, loss_g: 0.905\n",
            "[114/200]: loss_d: 1.272, loss_g: 0.905\n",
            "[115/200]: loss_d: 1.269, loss_g: 0.932\n",
            "[116/200]: loss_d: 1.259, loss_g: 0.924\n",
            "[117/200]: loss_d: 1.261, loss_g: 0.921\n",
            "[118/200]: loss_d: 1.265, loss_g: 0.914\n",
            "[119/200]: loss_d: 1.269, loss_g: 0.918\n",
            "[120/200]: loss_d: 1.268, loss_g: 0.908\n",
            "[121/200]: loss_d: 1.276, loss_g: 0.909\n",
            "[122/200]: loss_d: 1.276, loss_g: 0.904\n",
            "[123/200]: loss_d: 1.263, loss_g: 0.919\n",
            "[124/200]: loss_d: 1.271, loss_g: 0.902\n",
            "[125/200]: loss_d: 1.279, loss_g: 0.906\n",
            "[126/200]: loss_d: 1.269, loss_g: 0.921\n",
            "[127/200]: loss_d: 1.265, loss_g: 0.923\n",
            "[128/200]: loss_d: 1.269, loss_g: 0.916\n",
            "[129/200]: loss_d: 1.275, loss_g: 0.903\n",
            "[130/200]: loss_d: 1.274, loss_g: 0.902\n",
            "[131/200]: loss_d: 1.270, loss_g: 0.915\n",
            "[132/200]: loss_d: 1.271, loss_g: 0.909\n",
            "[133/200]: loss_d: 1.272, loss_g: 0.914\n",
            "[134/200]: loss_d: 1.274, loss_g: 0.904\n",
            "[135/200]: loss_d: 1.270, loss_g: 0.915\n",
            "[136/200]: loss_d: 1.272, loss_g: 0.910\n",
            "[137/200]: loss_d: 1.266, loss_g: 0.905\n",
            "[138/200]: loss_d: 1.274, loss_g: 0.902\n",
            "[139/200]: loss_d: 1.274, loss_g: 0.901\n",
            "[140/200]: loss_d: 1.262, loss_g: 0.922\n",
            "[141/200]: loss_d: 1.269, loss_g: 0.912\n",
            "[142/200]: loss_d: 1.273, loss_g: 0.902\n",
            "[143/200]: loss_d: 1.270, loss_g: 0.917\n",
            "[144/200]: loss_d: 1.271, loss_g: 0.912\n",
            "[145/200]: loss_d: 1.277, loss_g: 0.892\n",
            "[146/200]: loss_d: 1.274, loss_g: 0.899\n",
            "[147/200]: loss_d: 1.275, loss_g: 0.902\n",
            "[148/200]: loss_d: 1.275, loss_g: 0.901\n",
            "[149/200]: loss_d: 1.270, loss_g: 0.912\n",
            "[150/200]: loss_d: 1.271, loss_g: 0.895\n",
            "[151/200]: loss_d: 1.272, loss_g: 0.901\n",
            "[152/200]: loss_d: 1.277, loss_g: 0.899\n",
            "[153/200]: loss_d: 1.270, loss_g: 0.918\n",
            "[154/200]: loss_d: 1.270, loss_g: 0.903\n",
            "[155/200]: loss_d: 1.280, loss_g: 0.888\n",
            "[156/200]: loss_d: 1.276, loss_g: 0.904\n",
            "[157/200]: loss_d: 1.266, loss_g: 0.918\n",
            "[158/200]: loss_d: 1.267, loss_g: 0.901\n",
            "[159/200]: loss_d: 1.284, loss_g: 0.879\n",
            "[160/200]: loss_d: 1.282, loss_g: 0.893\n",
            "[161/200]: loss_d: 1.279, loss_g: 0.895\n",
            "[162/200]: loss_d: 1.279, loss_g: 0.891\n",
            "[163/200]: loss_d: 1.277, loss_g: 0.900\n",
            "[164/200]: loss_d: 1.278, loss_g: 0.900\n",
            "[165/200]: loss_d: 1.278, loss_g: 0.891\n",
            "[166/200]: loss_d: 1.277, loss_g: 0.895\n",
            "[167/200]: loss_d: 1.278, loss_g: 0.899\n",
            "[168/200]: loss_d: 1.268, loss_g: 0.912\n",
            "[169/200]: loss_d: 1.275, loss_g: 0.891\n",
            "[170/200]: loss_d: 1.277, loss_g: 0.903\n",
            "[171/200]: loss_d: 1.275, loss_g: 0.906\n",
            "[172/200]: loss_d: 1.275, loss_g: 0.890\n",
            "[173/200]: loss_d: 1.277, loss_g: 0.896\n",
            "[174/200]: loss_d: 1.273, loss_g: 0.908\n",
            "[175/200]: loss_d: 1.280, loss_g: 0.892\n",
            "[176/200]: loss_d: 1.271, loss_g: 0.890\n",
            "[177/200]: loss_d: 1.284, loss_g: 0.874\n",
            "[178/200]: loss_d: 1.284, loss_g: 0.887\n",
            "[179/200]: loss_d: 1.274, loss_g: 0.914\n",
            "[180/200]: loss_d: 1.274, loss_g: 0.897\n",
            "[181/200]: loss_d: 1.271, loss_g: 0.904\n",
            "[182/200]: loss_d: 1.281, loss_g: 0.878\n",
            "[183/200]: loss_d: 1.286, loss_g: 0.878\n",
            "[184/200]: loss_d: 1.277, loss_g: 0.898\n",
            "[185/200]: loss_d: 1.270, loss_g: 0.902\n",
            "[186/200]: loss_d: 1.274, loss_g: 0.897\n",
            "[187/200]: loss_d: 1.269, loss_g: 0.911\n",
            "[188/200]: loss_d: 1.284, loss_g: 0.884\n",
            "[189/200]: loss_d: 1.284, loss_g: 0.885\n",
            "[190/200]: loss_d: 1.284, loss_g: 0.885\n",
            "[191/200]: loss_d: 1.281, loss_g: 0.894\n",
            "[192/200]: loss_d: 1.277, loss_g: 0.889\n",
            "[193/200]: loss_d: 1.279, loss_g: 0.894\n",
            "[194/200]: loss_d: 1.286, loss_g: 0.879\n",
            "[195/200]: loss_d: 1.284, loss_g: 0.882\n",
            "[196/200]: loss_d: 1.279, loss_g: 0.901\n",
            "[197/200]: loss_d: 1.285, loss_g: 0.888\n",
            "[198/200]: loss_d: 1.280, loss_g: 0.884\n",
            "[199/200]: loss_d: 1.283, loss_g: 0.884\n",
            "[200/200]: loss_d: 1.279, loss_g: 0.889\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kn_1DNRVuIHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with torch.no_grad():\n",
        "    test_z = Variable(torch.randn(bs, z_dim).to(device))\n",
        "    generated = G(test_z)\n",
        "\n",
        "    save_image(generated.view(generated.size(0), 1, 28, 28), 'sample_data/sample_' + '.png')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPzwt_XoESLc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}